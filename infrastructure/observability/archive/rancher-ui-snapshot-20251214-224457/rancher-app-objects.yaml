apiVersion: v1
items:
- apiVersion: catalog.cattle.io/v1
  kind: App
  metadata:
    annotations:
      objectset.rio.cattle.io/applied: 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
      objectset.rio.cattle.io/id: helm-app
      objectset.rio.cattle.io/owner-gvk: /v1, Kind=Secret
      objectset.rio.cattle.io/owner-name: sh.helm.release.v1.rancher-monitoring.v1
      objectset.rio.cattle.io/owner-namespace: cattle-monitoring-system
    creationTimestamp: "2025-12-15T06:43:59Z"
    generation: 2
    labels:
      catalog.cattle.io/cluster-repo-name: rancher-charts
      objectset.rio.cattle.io/hash: afd0d9d7cfc6e6d7ab5c7044fb2bc771e55109c1
    name: rancher-monitoring
    namespace: cattle-monitoring-system
    ownerReferences:
    - apiVersion: v1
      blockOwnerDeletion: false
      controller: true
      kind: Secret
      name: sh.helm.release.v1.rancher-monitoring.v1
      uid: 2d0b3905-2570-4015-a0a4-3836f1cb9004
    resourceVersion: "65951595"
    uid: 474b8e94-f7f5-42b0-8f0c-39ff7d49a433
  spec:
    chart:
      metadata:
        annotations:
          artifacthub.io/license: Apache-2.0
          artifacthub.io/links: |
            - name: Chart Source
              url: https://github.com/prometheus-community/helm-charts
            - name: Upstream Project
              url: https://github.com/prometheus-operator/kube-prometheus
            - name: Upgrade Process
              url: https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/README.md#upgrading-chart
          artifacthub.io/operator: "true"
          catalog.cattle.io/auto-install: rancher-monitoring-crd=match
          catalog.cattle.io/certified: rancher
          catalog.cattle.io/deploys-on-os: windows
          catalog.cattle.io/display-name: Monitoring
          catalog.cattle.io/kube-version: '>= 1.32.0-0 < 1.35.0-0'
          catalog.cattle.io/namespace: cattle-monitoring-system
          catalog.cattle.io/permits-os: linux,windows
          catalog.cattle.io/provides-gvr: monitoring.coreos.com.prometheus/v1
          catalog.cattle.io/rancher-version: '>= 2.13.0-0 < 2.14.0-0'
          catalog.cattle.io/release-name: rancher-monitoring
          catalog.cattle.io/requests-cpu: 4500m
          catalog.cattle.io/requests-memory: 4000Mi
          catalog.cattle.io/type: cluster-tool
          catalog.cattle.io/ui-component: monitoring
          catalog.cattle.io/ui-source-repo: rancher-charts
          catalog.cattle.io/ui-source-repo-type: cluster
          catalog.cattle.io/upstream-version: 77.9.1
        apiVersion: v2
        appVersion: v0.85.0
        description: kube-prometheus-stack collects Kubernetes manifests, Grafana
          dashboards, and Prometheus rules combined with documentation and scripts
          to provide easy to operate end-to-end Kubernetes cluster monitoring with
          Prometheus using the Prometheus Operator.
        home: https://github.com/prometheus-operator/kube-prometheus
        icon: file://assets/logos/rancher-monitoring.png
        keywords:
        - operator
        - prometheus
        - kube-prometheus
        kubeVersion: '>=1.25.0-0'
        maintainers:
        - email: andrew@quadcorps.co.uk
          name: andrewgkew
          url: https://github.com/andrewgkew
        - email: gianrubio@gmail.com
          name: gianrubio
          url: https://github.com/gianrubio
        - email: github.gkarthiks@gmail.com
          name: gkarthiks
          url: https://github.com/gkarthiks
        - email: kube-prometheus-stack@sisti.pt
          name: GMartinez-Sisti
          url: https://github.com/GMartinez-Sisti
        - email: github@jkroepke.de
          name: jkroepke
          url: https://github.com/jkroepke
        - email: scott@r6by.com
          name: scottrigby
          url: https://github.com/scottrigby
        - email: miroslav.hadzhiev@gmail.com
          name: Xtigyro
          url: https://github.com/Xtigyro
        - email: quentin.bisson@gmail.com
          name: QuentinBisson
          url: https://github.com/QuentinBisson
        name: rancher-monitoring
        sources:
        - https://github.com/prometheus-community/helm-charts
        - https://github.com/prometheus-operator/kube-prometheus
        type: application
        version: 108.0.0+up77.9.1-rancher.6
    helmVersion: 3
    info:
      description: Install complete
      firstDeployed: "2025-12-15T06:43:49Z"
      lastDeployed: "2025-12-15T06:43:49Z"
      notes: |
        rancher-monitoring has been installed. Check its status by running:
          kubectl --namespace cattle-monitoring-system get pods -l "release=rancher-monitoring"

        Get Grafana 'admin' user password by running:

          kubectl --namespace cattle-monitoring-system get secrets rancher-monitoring-grafana -o jsonpath="{.data.admin-password}" | base64 -d ; echo

        Access Grafana local instance:

          export POD_NAME=$(kubectl --namespace cattle-monitoring-system get pod -l "app.kubernetes.io/name=grafana,app.kubernetes.io/instance=rancher-monitoring" -oname)
          kubectl --namespace cattle-monitoring-system port-forward $POD_NAME 3000

        Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create & configure Alertmanager and Prometheus instances using the Operator.
      readme: |
        # kube-prometheus-stack

        Installs core components of the [kube-prometheus stack](https://github.com/prometheus-operator/kube-prometheus), a collection of Kubernetes manifests, [Grafana](http://grafana.com/) dashboards, and [Prometheus rules](https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/) combined with documentation and scripts to provide easy to operate end-to-end Kubernetes cluster monitoring with [Prometheus](https://prometheus.io/) using the [Prometheus Operator](https://github.com/prometheus-operator/prometheus-operator).

        See the [kube-prometheus](https://github.com/prometheus-operator/kube-prometheus) readme for details about components, dashboards, and alerts.

        _Note: This chart was formerly named `prometheus-operator` chart, now renamed to more clearly reflect that it installs the `kube-prometheus` project stack, within which Prometheus Operator is only one component. This chart does not install all components of `kube-prometheus`, notably excluding the Prometheus Adapter and Prometheus black-box exporter._

        ## Prerequisites

        - Kubernetes 1.19+
        - Helm 3+

        ## Usage

        The chart is distributed as an [OCI Artifact](https://helm.sh/docs/topics/registries/) as well as via a traditional [Helm Repository](https://helm.sh/docs/topics/chart_repository/).

        - OCI Artifact: `oci://ghcr.io/prometheus-community/charts/kube-prometheus-stack`
        - Helm Repository: `https://prometheus-community.github.io/helm-charts` with chart `kube-prometheus-stack`

        The installation instructions use the OCI registry. Refer to the [`helm repo`]([`helm repo`](https://helm.sh/docs/helm/helm_repo/)) command documentation for information on installing charts via the traditional repository.

        ### Install Helm Chart

        ```console
        helm install [RELEASE_NAME] oci://ghcr.io/prometheus-community/charts/kube-prometheus-stack
        ```

        _See [configuration](#configuration) below._

        _See [helm install](https://helm.sh/docs/helm/helm_install/) for command documentation._

        ### Dependencies

        By default this chart installs additional, dependent charts:

        - [prometheus-community/kube-state-metrics](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-state-metrics)
        - [prometheus-community/prometheus-node-exporter](https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-node-exporter)
        - [grafana/grafana](https://github.com/grafana/helm-charts/tree/main/charts/grafana)

        To disable dependencies during installation, see [multiple releases](#multiple-releases) below.

        _See [helm dependency](https://helm.sh/docs/helm/helm_dependency/) for command documentation._

        #### Grafana Dashboards

        This chart provisions a collection of curated Grafana dashboards that are automatically loaded into Grafana via ConfigMaps. These dashboards are rendered into the Helm chart under [`templates/grafana/`](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack/templates/grafana/), but **this is not their source of truth**.

        The dashboards originate from various upstream projects and are gathered and processed using scripts in the [`hack/`](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack/hack) directory. For details on how these dashboards are sourced and kept up to date, refer to the [hack/README.md](https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/hack/README.md).

        > **Note:** The dashboards referenced in the `hack` scripts are usually **not the original source** either. Most originate from separate **Prometheus mixin repositories** (e.g., [kubernetes-mixin](https://github.com/kubernetes-monitoring/kubernetes-mixin)) and are processed through `jsonnet` tooling before being included here. To find the original source in case you want to modify it you may have to search even further upstream.

        If you wish to contribute or modify dashboards, please follow the guidance in the `hack/README.md` to ensure consistency and reproducibility.

        ### Uninstall Helm Chart

        ```console
        helm uninstall [RELEASE_NAME]
        ```

        This removes all the Kubernetes components associated with the chart and deletes the release.

        _See [helm uninstall](https://helm.sh/docs/helm/helm_uninstall/) for command documentation._

        CRDs created by this chart are not removed by default and should be manually cleaned up:

        ```console
        kubectl delete crd alertmanagerconfigs.monitoring.coreos.com
        kubectl delete crd alertmanagers.monitoring.coreos.com
        kubectl delete crd podmonitors.monitoring.coreos.com
        kubectl delete crd probes.monitoring.coreos.com
        kubectl delete crd prometheusagents.monitoring.coreos.com
        kubectl delete crd prometheuses.monitoring.coreos.com
        kubectl delete crd prometheusrules.monitoring.coreos.com
        kubectl delete crd scrapeconfigs.monitoring.coreos.com
        kubectl delete crd servicemonitors.monitoring.coreos.com
        kubectl delete crd thanosrulers.monitoring.coreos.com
        ```

        ### Upgrading Chart

        ```console
        helm upgrade [RELEASE_NAME] [CHART]
        ```

        With Helm v3, CRDs created by this chart are not updated by default and should be manually updated.
        Consult also the [Helm Documentation on CRDs](https://helm.sh/docs/chart_best_practices/custom_resource_definitions).

        CRDs update lead to a major version bump.
        The Chart's [appVersion](https://github.com/prometheus-community/helm-charts/blob/13ed7098db2f78c2bbcdab6c1c3c7a95b4b94574/charts/kube-prometheus-stack/Chart.yaml#L36) refers to the [`prometheus-operator`](https://github.com/prometheus-operator/prometheus-operator/tree/main)'s version with matching CRDs.

        _See [helm upgrade](https://helm.sh/docs/helm/helm_upgrade/) for command documentation._

        #### Upgrading an existing Release to a new major version

        A major chart version change (like v1.2.3 -> v2.0.0) indicates that there is an incompatible breaking change needing manual actions.

        See [UPGRADE.md](https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/UPGRADE.md)
        for breaking changes between versions.

        ## Configuration

        See [Customizing the Chart Before Installing](https://helm.sh/docs/intro/using_helm/#customizing-the-chart-before-installing). To see all configurable options with detailed comments:

        ```console
        helm show values oci://ghcr.io/prometheus-community/charts/kube-prometheus-stack
        ```

        You may also run `helm show values` on this chart's [dependencies](#dependencies) for additional options.

        ### Rancher Monitoring Configuration

        The following table shows values exposed by Rancher Monitoring's additions to the chart:

        | Parameter | Description | Default |
        | ----- | ----------- | ------ |
        | `nameOverride` | Provide a name that should be used instead of the chart name when naming all resources deployed by this chart |`"rancher-monitoring"`|
        | `namespaceOverride` | Override the deployment namespace | `"cattle-monitoring-system"` |
        | `global.rbac.userRoles.create` | Create default user ClusterRoles to allow users to interact with Prometheus CRs, ConfigMaps, and Secrets | `true` |
        | `global.rbac.userRoles.aggregateToDefaultRoles` | Aggregate default user ClusterRoles into default k8s ClusterRoles | `true` |
        | `prometheus-adapter.enabled` | Whether to install [prometheus-adapter](https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-adapter) within the cluster | `true` |
        | `prometheus-adapter.prometheus.url` | A URL pointing to the Prometheus deployment within your cluster. The default value is set based on the assumption that you plan to deploy the default Prometheus instance from this chart where `.Values.namespaceOverride=cattle-monitoring-system` and `.Values.nameOverride=rancher-monitoring` | `http://rancher-monitoring-prometheus.cattle-monitoring-system.svc` |
        | `prometheus-adapter.prometheus.port` | The port on the Prometheus deployment that Prometheus Adapter can make requests to | `9090` |
        | `prometheus.prometheusSpec.ignoreNamespaceSelectors` | Ignore NamespaceSelector settings from the PodMonitor and ServiceMonitor configs. If true, PodMonitors and ServiceMonitors can only discover Pods and Services within the namespace they are deployed into | `false` |

        The following values must be set when installing the chart manually. They/these values are automatically populated when installed via the UI.
        Even with these values, manual installation may still have problems. It is strongly recommended to install via the UI, as this is the only supported method by the Rancher Team.

        | Parameter | Description | Default |
        | ----- | ----------- | ------ |
        | `global.cattle.clusterId` | The cluster ID, required for Grafana ingress to work correctly. For local/upstream clusters, the value must be `local`. For Downstream Clusters the value needs to be the ID of the cluster, which you can get by using the following command: `kubectl get clusters.provisioning.cattle.io -n fleet-default CLUSTER_NAME -o=jsonpath="{.status.clusterName}"` or going to the UI and in the Cluster Management, find the cluster in the list and from the right-hand three-dot menu, choose 'View in API' - the cluster id is in the 'status.clusterName' field   | `local` |
        | `global.cattle.clusterName` | The cluster name, usually `local` for upstream clusters. For downstream clusters the name can be seen on Cluster Management on the clusters list | `local` |


        The following values are enabled for different distributions via [rancher-pushprox](https://github.com/rancher/dev-charts/tree/master/packages/rancher-pushprox). See the rancher-pushprox `README.md` for more information on what all values can be configured for the PushProxy chart.

        | Parameter | Description | Default |
        | ----- | ----------- | ------ |
        | `rkeControllerManager.enabled` | Create a PushProx installation for monitoring kube-controller-manager metrics in RKE clusters | `false` |
        | `rkeScheduler.enabled` | Create a PushProx installation for monitoring kube-scheduler metrics in RKE clusters | `false` |
        | `rkeProxy.enabled` | Create a PushProx installation for monitoring kube-proxy metrics in RKE clusters | `false` |
        | `rkeIngressNginx.enabled` | Create a PushProx installation for monitoring ingress-nginx metrics in RKE clusters | `false` |
        | `rkeEtcd.enabled` | Create a PushProx installation for monitoring etcd metrics in RKE clusters | `false` |
        | `rke2IngressNginx.enabled` | Create a PushProx installation for monitoring ingress-nginx metrics in RKE2 clusters | `false` |
        | `k3sServer.enabled` | Create a PushProx installation for monitoring k3s-server metrics (accounts for kube-controller-manager, kube-scheduler, and kube-proxy metrics) in k3s clusters | `false` |
        | `kubeAdmControllerManager.enabled` | Create a PushProx installation for monitoring kube-controller-manager metrics in kubeAdm clusters | `false` |
        | `kubeAdmScheduler.enabled` | Create a PushProx installation for monitoring kube-scheduler metrics in kubeAdm clusters | `false` |
        | `kubeAdmProxy.enabled` | Create a PushProx installation for monitoring kube-proxy metrics in kubeAdm clusters | `false` |
        | `kubeAdmEtcd.enabled` | Create a PushProx installation for monitoring etcd metrics in kubeAdm clusters | `false` |

        ### Multiple releases

        The same chart can be used to run multiple Prometheus instances in the same cluster if required. To achieve this, it is necessary to run only one instance of prometheus-operator and a pair of alertmanager pods for an HA configuration, while all other components need to be disabled. To disable a dependency during installation, set `kubeStateMetrics.enabled`, `nodeExporter.enabled` and `grafana.enabled` to `false`.

        ## Work-Arounds for Known Issues

        ### Running on private GKE clusters

        When Google configure the control plane for private clusters, they automatically configure VPC peering between your Kubernetes clusterâ€™s network and a separate Google managed project. In order to restrict what Google are able to access within your cluster, the firewall rules configured restrict access to your Kubernetes pods. This means that in order to use the webhook component with a GKE private cluster, you must configure an additional firewall rule to allow the GKE control plane access to your webhook pod.

        You can read more information on how to add firewall rules for the GKE control plane nodes in the [GKE docs](https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters#add_firewall_rules)

        Alternatively, you can disable the hooks by setting `prometheusOperator.admissionWebhooks.enabled=false`.

        ## PrometheusRules Admission Webhooks

        With Prometheus Operator version 0.30+, the core Prometheus Operator pod exposes an endpoint that will integrate with the `validatingwebhookconfiguration` Kubernetes feature to prevent malformed rules from being added to the cluster.

        ### How the Chart Configures the Hooks

        A validating and mutating webhook configuration requires the endpoint to which the request is sent to use TLS. It is possible to set up custom certificates to do this, but in most cases, a self-signed certificate is enough. The setup of this component requires some more complex orchestration when using helm. The steps are created to be idempotent and to allow turning the feature on and off without running into helm quirks.

        1. A pre-install hook provisions a certificate into the same namespace using a format compatible with provisioning using end user certificates. If the certificate already exists, the hook exits.
        2. The prometheus operator pod is configured to use a TLS proxy container, which will load that certificate.
        3. Validating and Mutating webhook configurations are created in the cluster, with their failure mode set to Ignore. This allows rules to be created by the same chart at the same time, even though the webhook has not yet been fully set up - it does not have the correct CA field set.
        4. A post-install hook reads the CA from the secret created by step 1 and patches the Validating and Mutating webhook configurations. This process will allow a custom CA provisioned by some other process to also be patched into the webhook configurations. The chosen failure policy is also patched into the webhook configurations

        ### Alternatives

        It should be possible to use [jetstack/cert-manager](https://github.com/jetstack/cert-manager) if a more complete solution is required, but it has not been tested.

        You can enable automatic self-signed TLS certificate provisioning via cert-manager by setting the `prometheusOperator.admissionWebhooks.certManager.enabled` value to true.

        ### Limitations

        Because the operator can only run as a single pod, there is potential for this component failure to cause rule deployment failure. Because this risk is outweighed by the benefit of having validation, the feature is enabled by default.

        ## Developing Prometheus Rules and Grafana Dashboards

        This chart Grafana Dashboards and Prometheus Rules are just a copy from [prometheus-operator/prometheus-operator](https://github.com/prometheus-operator/prometheus-operator) and other sources, synced (with alterations) by scripts in [hack](hack) folder. In order to introduce any changes you need to first [add them to the original repository](https://github.com/prometheus-operator/kube-prometheus/blob/main/docs/customizations/developing-prometheus-rules-and-grafana-dashboards.md) and then sync there by scripts.

        ## Further Information

        For more in-depth documentation of configuration options meanings, please see

        - [Prometheus Operator](https://github.com/prometheus-operator/prometheus-operator)
        - [Prometheus](https://prometheus.io/docs/introduction/overview/)
        - [Grafana](https://github.com/grafana/helm-charts/tree/main/charts/grafana#grafana-helm-chart)

        ## prometheus.io/scrape

        The prometheus operator does not support annotation-based discovery of services, using the `PodMonitor` or `ServiceMonitor` CRD in its place as they provide far more configuration options.
        For information on how to use PodMonitors/ServiceMonitors, please see the documentation on the `prometheus-operator/prometheus-operator` documentation here:

        - [ServiceMonitors](https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/developer/getting-started.md#using-servicemonitors)
        - [PodMonitors](https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/developer/getting-started.md#using-podmonitors)
        - [Running Exporters](https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/running-exporters.md)

        By default, Prometheus discovers PodMonitors and ServiceMonitors within its namespace, that are labeled with the same release tag as the prometheus-operator release.
        Sometimes, you may need to discover custom PodMonitors/ServiceMonitors, for example used to scrape data from third-party applications.
        An easy way of doing this, without compromising the default PodMonitors/ServiceMonitors discovery, is allowing Prometheus to discover all PodMonitors/ServiceMonitors within its namespace, without applying label filtering.
        To do so, you can set `prometheus.prometheusSpec.podMonitorSelectorNilUsesHelmValues` and `prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues` to `false`.

        ## Migrating from stable/prometheus-operator chart

        ## Zero downtime

        Since `kube-prometheus-stack` is fully compatible with the `stable/prometheus-operator` chart, a migration without downtime can be achieved.
        However, the old name prefix needs to be kept. If you want the new name please follow the step by step guide below (with downtime).

        You can override the name to achieve this:

        ```console
        helm upgrade prometheus-operator prometheus-community/kube-prometheus-stack -n monitoring --reuse-values --set nameOverride=prometheus-operator
        ```

        **Note**: It is recommended to run this first with `--dry-run --debug`.

        ## Redeploy with new name (downtime)

        If the **prometheus-operator** values are compatible with the new **kube-prometheus-stack** chart, please follow the below steps for migration:

        > The guide presumes that chart is deployed in `monitoring` namespace and the deployments are running there. If in other namespace, please replace the `monitoring` to the deployed namespace.

        1. Patch the PersistenceVolume created/used by the prometheus-operator chart to `Retain` claim policy:

            ```console
            kubectl patch pv/<PersistentVolume name> -p '{"spec":{"persistentVolumeReclaimPolicy":"Retain"}}'
            ```

            **Note:** To execute the above command, the user must have a cluster wide permission. Please refer [Kubernetes RBAC](https://kubernetes.io/docs/reference/access-authn-authz/rbac/)

        2. Uninstall the **prometheus-operator** release and delete the existing PersistentVolumeClaim, and verify PV become Released.

            ```console
            helm uninstall prometheus-operator -n monitoring
            kubectl delete pvc/<PersistenceVolumeClaim name> -n monitoring
            ```

            Additionally, you have to manually remove the remaining `prometheus-operator-kubelet` service.

            ```console
            kubectl delete service/prometheus-operator-kubelet -n kube-system
            ```

            You can choose to remove all your existing CRDs (ServiceMonitors, Podmonitors, etc.) if you want to.

        3. Remove current `spec.claimRef` values to change the PV's status from Released to Available.

            ```console
            kubectl patch pv/<PersistentVolume name> --type json -p='[{"op": "remove", "path": "/spec/claimRef"}]' -n monitoring
            ```

        **Note:** To execute the above command, the user must have a cluster wide permission. Please refer to [Kubernetes RBAC](https://kubernetes.io/docs/reference/access-authn-authz/rbac/)

        After these steps, proceed to a fresh **kube-prometheus-stack** installation and make sure the current release of **kube-prometheus-stack** matching the `volumeClaimTemplate` values in the `values.yaml`.

        The binding is done via matching a specific amount of storage requested and with certain access modes.

        For example, if you had storage specified as this with **prometheus-operator**:

        ```yaml
        volumeClaimTemplate:
          spec:
            storageClassName: gp2
            accessModes: ["ReadWriteOnce"]
            resources:
             requests:
               storage: 50Gi
        ```

        You have to specify matching `volumeClaimTemplate` with 50Gi storage and `ReadWriteOnce` access mode.

        Additionally, you should check the current AZ of your legacy installation's PV, and configure the fresh release to use the same AZ as the old one. If the pods are in a different AZ than the PV, the release will fail to bind the existing one, hence creating a new PV.

        This can be achieved either by specifying the labels through `values.yaml`, e.g. setting `prometheus.prometheusSpec.nodeSelector` to:

        ```yaml
        nodeSelector:
          failure-domain.beta.kubernetes.io/zone: east-west-1a
        ```

        or passing these values as `--set` overrides during installation.

        The new release should now re-attach your previously released PV with its content.

        ## Migrating from coreos/prometheus-operator chart

        The multiple charts have been combined into a single chart that installs prometheus operator, prometheus, alertmanager, grafana as well as the multitude of exporters necessary to monitor a cluster.

        There is no simple and direct migration path between the charts as the changes are extensive and intended to make the chart easier to support.

        The capabilities of the old chart are all available in the new chart, including the ability to run multiple prometheus instances on a single cluster - you will need to disable the parts of the chart you do not wish to deploy.

        You can check out the tickets for this change at [prometheus-operator/prometheus-operator #592](https://github.com/prometheus-operator/prometheus-operator/issues/592) and [helm/charts #6765](https://github.com/helm/charts/pull/6765).

        ### High-level overview of Changes

        #### Added dependencies

        The chart has added 3 [dependencies](#dependencies).

        - Node-Exporter, Kube-State-Metrics: These components are loaded as dependencies into the chart, and are relatively simple components
        - Grafana: The Grafana chart is more feature-rich than this chart - it contains a sidecar that is able to load data sources and dashboards from configmaps deployed into the same cluster. For more information check out the [documentation for the chart](https://github.com/grafana/helm-charts/blob/main/charts/grafana/README.md)

        #### Kubelet Service

        Because the kubelet service has a new name in the chart, make sure to clean up the old kubelet service in the `kube-system` namespace to prevent counting container metrics twice.

        #### Persistent Volumes

        If you would like to keep the data of the current persistent volumes, it should be possible to attach existing volumes to new PVCs and PVs that are created using the conventions in the new chart. For example, in order to use an existing Azure disk for a helm release called `prometheus-migration` the following resources can be created:

        ```yaml
        apiVersion: v1
        kind: PersistentVolume
        metadata:
          name: pvc-prometheus-migration-prometheus-0
        spec:
          accessModes:
          - ReadWriteOnce
          azureDisk:
            cachingMode: None
            diskName: pvc-prometheus-migration-prometheus-0
            diskURI: /subscriptions/f5125d82-2622-4c50-8d25-3f7ba3e9ac4b/resourceGroups/sample-migration-resource-group/providers/Microsoft.Compute/disks/pvc-prometheus-migration-prometheus-0
            fsType: ""
            kind: Managed
            readOnly: false
          capacity:
            storage: 1Gi
          persistentVolumeReclaimPolicy: Delete
          storageClassName: prometheus
          volumeMode: Filesystem
        ```

        ```yaml
        apiVersion: v1
        kind: PersistentVolumeClaim
        metadata:
          labels:
            app.kubernetes.io/name: prometheus
            prometheus: prometheus-migration-prometheus
          name: prometheus-prometheus-migration-prometheus-db-prometheus-prometheus-migration-prometheus-0
          namespace: monitoring
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: 1Gi
          storageClassName: prometheus
          volumeMode: Filesystem
          volumeName: pvc-prometheus-migration-prometheus-0
        ```

        The PVC will take ownership of the PV and when you create a release using a persistent volume claim template it will use the existing PVCs as they match the naming convention used by the chart. For other cloud providers similar approaches can be used.

        #### KubeProxy

        The metrics bind address of kube-proxy is default to `127.0.0.1:10249` that prometheus instances **cannot** access to. You should expose metrics by changing `metricsBindAddress` field value to `0.0.0.0:10249` if you want to collect them.

        Depending on the cluster, the relevant part `config.conf` will be in ConfigMap `kube-system/kube-proxy` or `kube-system/kube-proxy-config`. For example:

        ```console
        kubectl -n kube-system edit cm kube-proxy
        ```

        ```yaml
        apiVersion: v1
        data:
          config.conf: |-
            apiVersion: kubeproxy.config.k8s.io/v1alpha1
            kind: KubeProxyConfiguration
            # ...
            # metricsBindAddress: 127.0.0.1:10249
            metricsBindAddress: 0.0.0.0:10249
            # ...
          kubeconfig.conf: |-
            # ...
        kind: ConfigMap
        metadata:
          labels:
            app: kube-proxy
          name: kube-proxy
          namespace: kube-system
        ```
      status: deployed
    name: rancher-monitoring
    namespace: cattle-monitoring-system
    resources:
    - apiVersion: v1
      kind: Namespace
      name: cattle-dashboards
    - apiVersion: networking.k8s.io/v1
      kind: NetworkPolicy
      name: rancher-monitoring-coredns-allow-all
      namespace: kube-system
    - apiVersion: v1
      kind: ServiceAccount
      name: rancher-monitoring-grafana
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ServiceAccount
      name: rancher-monitoring-kube-state-metrics
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ServiceAccount
      name: rancher-monitoring-prometheus-adapter
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ServiceAccount
      name: rancher-monitoring-prometheus-node-exporter
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ServiceAccount
      name: pushprox-kube-controller-manager-client
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ServiceAccount
      name: pushprox-kube-controller-manager-proxy
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ServiceAccount
      name: pushprox-kube-etcd-client
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ServiceAccount
      name: pushprox-kube-etcd-proxy
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ServiceAccount
      name: pushprox-kube-proxy-client
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ServiceAccount
      name: pushprox-kube-proxy-proxy
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ServiceAccount
      name: pushprox-kube-scheduler-client
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ServiceAccount
      name: pushprox-kube-scheduler-proxy
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ServiceAccount
      name: rancher-monitoring-windows-exporter
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ServiceAccount
      name: rancher-monitoring-alertmanager
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ServiceAccount
      name: rancher-monitoring-operator
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ServiceAccount
      name: rancher-monitoring-prometheus
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ServiceAccount
      name: rancher-monitoring-patch-sa
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: Secret
      name: rancher-monitoring-grafana
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: Secret
      name: pushprox-kube-controller-manager-client-service-account-token
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: Secret
      name: pushprox-kube-scheduler-client-service-account-token
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-grafana-config-dashboards
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-grafana
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ConfigMap
      name: grafana-nginx-proxy-config
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-prometheus-adapter
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-windows-exporter
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-windows-exporter-scripts
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-grafana-datasource
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-alertmanager-overview
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-apiserver
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-cluster-total
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-controller-manager
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-grafana-overview
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-k8s-coredns
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-k8s-resources-cluster
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-k8s-resources-multicluster
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-k8s-resources-namespace
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-k8s-resources-node
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-k8s-resources-pod
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-k8s-resources-workload
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-k8s-resources-workloads-namespace
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-kubelet
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-namespace-by-pod
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-namespace-by-workload
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-node-cluster-rsrc-use
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-node-rsrc-use
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-nodes-aix
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-nodes-darwin
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-nodes
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-persistentvolumesusage
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-pod-total
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-prometheus
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-workload-total
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: prometheus-nginx-proxy-config
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-default-dashboards-cluster
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-default-dashboards-home
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-fleet-dashboards
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-default-dashboards-k8s
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-default-dashboards-nodes
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-default-dashboards-pods
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-default-dashboards-workloads
      namespace: cattle-dashboards
    - apiVersion: v1
      kind: PersistentVolumeClaim
      name: rancher-monitoring-grafana
      namespace: cattle-monitoring-system
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: rancher-monitoring-grafana-clusterrole
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: rancher-monitoring-kube-state-metrics
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: prometheus-adapter-resource-reader
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: prometheus-adapter-server-resources
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: pushprox-kube-controller-manager-client
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: pushprox-kube-controller-manager-proxy
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: pushprox-kube-etcd-client
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: pushprox-kube-etcd-proxy
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: pushprox-kube-proxy-client
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: pushprox-kube-proxy-proxy
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: pushprox-kube-scheduler-client
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: pushprox-kube-scheduler-proxy
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: rancher-monitoring-operator
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: rancher-monitoring-prometheus
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: monitoring-admin
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: monitoring-edit
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: monitoring-view
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: monitoring-ui-view
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: rancher-monitoring-patch-sa
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: rancher-monitoring-grafana-clusterrolebinding
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: rancher-monitoring-kube-state-metrics
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: prometheus-adapter-system-auth-delegator
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: prometheus-adapter-resource-reader
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: prometheus-adapter-hpa-controller
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: pushprox-kube-controller-manager-client
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: pushprox-kube-controller-manager-proxy
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: pushprox-kube-etcd-client
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: pushprox-kube-etcd-proxy
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: pushprox-kube-proxy-client
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: pushprox-kube-proxy-proxy
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: pushprox-kube-scheduler-client
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: pushprox-kube-scheduler-proxy
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: rancher-monitoring-operator
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: rancher-monitoring-prometheus
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: rancher-monitoring-patch-sa
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: Role
      name: rancher-monitoring-grafana
      namespace: cattle-monitoring-system
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: Role
      name: monitoring-config-admin
      namespace: cattle-monitoring-system
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: Role
      name: monitoring-config-edit
      namespace: cattle-monitoring-system
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: Role
      name: monitoring-config-view
      namespace: cattle-monitoring-system
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: Role
      name: monitoring-dashboard-admin
      namespace: cattle-dashboards
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: Role
      name: monitoring-dashboard-edit
      namespace: cattle-dashboards
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: Role
      name: monitoring-dashboard-view
      namespace: cattle-dashboards
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: RoleBinding
      name: rancher-monitoring-grafana
      namespace: cattle-monitoring-system
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: RoleBinding
      name: prometheus-adapter-auth-reader
      namespace: kube-system
    - apiVersion: v1
      kind: Service
      name: rancher-monitoring-grafana
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: Service
      name: rancher-monitoring-kube-state-metrics
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: Service
      name: rancher-monitoring-prometheus-adapter
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: Service
      name: rancher-monitoring-prometheus-node-exporter
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: Service
      name: pushprox-kube-controller-manager-proxy
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: Service
      name: pushprox-kube-controller-manager-client
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: Service
      name: pushprox-kube-etcd-proxy
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: Service
      name: pushprox-kube-etcd-client
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: Service
      name: pushprox-ingress-nginx-client
      namespace: kube-system
    - apiVersion: v1
      kind: Service
      name: pushprox-kube-proxy-proxy
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: Service
      name: pushprox-kube-proxy-client
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: Service
      name: pushprox-kube-scheduler-proxy
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: Service
      name: pushprox-kube-scheduler-client
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: Service
      name: rancher-monitoring-windows-exporter
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: Service
      name: rancher-monitoring-alertmanager
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: Service
      name: rancher-monitoring-coredns
      namespace: kube-system
    - apiVersion: v1
      kind: Service
      name: rancher-monitoring-operator
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: Service
      name: rancher-monitoring-prometheus
      namespace: cattle-monitoring-system
    - apiVersion: apps/v1
      kind: DaemonSet
      name: rancher-monitoring-prometheus-node-exporter
      namespace: cattle-monitoring-system
    - apiVersion: apps/v1
      kind: DaemonSet
      name: pushprox-kube-controller-manager-client
      namespace: cattle-monitoring-system
    - apiVersion: apps/v1
      kind: DaemonSet
      name: pushprox-kube-etcd-client
      namespace: cattle-monitoring-system
    - apiVersion: apps/v1
      kind: DaemonSet
      name: pushprox-kube-proxy-client
      namespace: cattle-monitoring-system
    - apiVersion: apps/v1
      kind: DaemonSet
      name: pushprox-kube-scheduler-client
      namespace: cattle-monitoring-system
    - apiVersion: apps/v1
      kind: DaemonSet
      name: rancher-monitoring-windows-exporter
      namespace: cattle-monitoring-system
    - apiVersion: apps/v1
      kind: Deployment
      name: rancher-monitoring-grafana
      namespace: cattle-monitoring-system
    - apiVersion: apps/v1
      kind: Deployment
      name: rancher-monitoring-kube-state-metrics
      namespace: cattle-monitoring-system
    - apiVersion: apps/v1
      kind: Deployment
      name: rancher-monitoring-prometheus-adapter
      namespace: cattle-monitoring-system
    - apiVersion: apps/v1
      kind: Deployment
      name: pushprox-kube-controller-manager-proxy
      namespace: cattle-monitoring-system
    - apiVersion: apps/v1
      kind: Deployment
      name: pushprox-kube-etcd-proxy
      namespace: cattle-monitoring-system
    - apiVersion: apps/v1
      kind: Deployment
      name: pushprox-kube-proxy-proxy
      namespace: cattle-monitoring-system
    - apiVersion: apps/v1
      kind: Deployment
      name: pushprox-kube-scheduler-proxy
      namespace: cattle-monitoring-system
    - apiVersion: apps/v1
      kind: Deployment
      name: rancher-monitoring-operator
      namespace: cattle-monitoring-system
    - apiVersion: apiregistration.k8s.io/v1
      kind: APIService
      name: v1beta1.custom.metrics.k8s.io
    - apiVersion: monitoring.coreos.com/v1
      kind: Alertmanager
      name: rancher-monitoring-alertmanager
      namespace: cattle-monitoring-system
    - apiVersion: admissionregistration.k8s.io/v1
      kind: MutatingWebhookConfiguration
      name: rancher-monitoring-admission
    - apiVersion: monitoring.coreos.com/v1
      kind: Prometheus
      name: rancher-monitoring-prometheus
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-alertmanager.rules
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-config-reloaders
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-etcd
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-general.rules
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-k8s.rules.container-cpu-usage-seconds-total
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-k8s.rules.container-memory-cache
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-k8s.rules.container-memory-rss
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-k8s.rules.container-memory-swap
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-k8s.rules.container-memory-working-set-bytes
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-k8s.rules.container-resource
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-k8s.rules.pod-owner
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-kube-apiserver-availability.rules
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-kube-apiserver-burnrate.rules
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-kube-apiserver-histogram.rules
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-kube-apiserver-slos
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-kube-prometheus-general.rules
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-kube-prometheus-node-recording.rules
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-kube-state-metrics
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-kubelet.rules
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-kubernetes-apps
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-kubernetes-resources
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-kubernetes-storage
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-kubernetes-system-apiserver
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-kubernetes-system-controller-manager
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-kubernetes-system-kubelet
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-kubernetes-system
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-node-exporter.rules
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-node-exporter
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-node-network
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-node.rules
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-prometheus-operator
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: PrometheusRule
      name: rancher-monitoring-prometheus
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: ServiceMonitor
      name: rancher-monitoring-grafana
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: ServiceMonitor
      name: rancher-monitoring-kube-state-metrics
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: ServiceMonitor
      name: rancher-monitoring-prometheus-node-exporter
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: ServiceMonitor
      name: rancher-monitoring-kube-controller-manager
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: ServiceMonitor
      name: rancher-monitoring-kube-etcd
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: ServiceMonitor
      name: rancher-monitoring-ingress-nginx
      namespace: kube-system
    - apiVersion: monitoring.coreos.com/v1
      kind: ServiceMonitor
      name: rancher-monitoring-kube-proxy
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: ServiceMonitor
      name: rancher-monitoring-kube-scheduler
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: ServiceMonitor
      name: rancher-monitoring-windows-exporter
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: ServiceMonitor
      name: rancher-monitoring-alertmanager
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: ServiceMonitor
      name: rancher-monitoring-coredns
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: ServiceMonitor
      name: rancher-monitoring-apiserver
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: ServiceMonitor
      name: rancher-monitoring-kubelet
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: ServiceMonitor
      name: rancher-monitoring-operator
      namespace: cattle-monitoring-system
    - apiVersion: monitoring.coreos.com/v1
      kind: ServiceMonitor
      name: rancher-monitoring-prometheus
      namespace: cattle-monitoring-system
    - apiVersion: admissionregistration.k8s.io/v1
      kind: ValidatingWebhookConfiguration
      name: rancher-monitoring-admission
    version: 1
  status:
    observedGeneration: 2
    summary:
      state: deployed
- apiVersion: catalog.cattle.io/v1
  kind: App
  metadata:
    annotations:
      objectset.rio.cattle.io/applied: H4sIAAAAAAAA/6xVTY/jNgz9KwZ7rKz1V5wZAz1sd4CiKIoC6ceh7R5omY7VyJIhyS7SwP+9kLOTnclkJsG2twSk+Mj3HukD9OSxQY9QHQC1Nh69NNqFv6b+i4R35LmVhgv0XhGX5p1soIKOVB/jMAB7Nc/8rcnG22kHFbybUhb9IHXzzc8kLPmrzzT2BBW4jgckbkkROuJTyi1q0ZGNe6OlN1bqbSxsw6fipppuQBEKH2NPi7i989TDzEBhTWqhQKBHZbZPKgk1Ok82tjSYxyYfOxIdWu/eaKND10EFeVGXeVtmLeb1fZ7UNdVZKVZlk92vMbsTeZvW9ySS0MsZxPOh4Ri/OhODhYANtWRJC3JQ/XEAHORvZJ00GiqYUmBQKyN2P4XUB1Lkl0iLyhEDYbS3RimyUHk7EoOd1MEIJz2/TLFxcVMjapEURRlnbbKKiwKL+A5FGq9pVWBetu2aUpg/zgzcQGJRJnAdfrxh4AvqkfWyldR8phTYhbxONg0FWsKoFzNu5P3lw0+8nHvnTNiZncmTAYOGnLByOOoC32vnUSkX+Y6iD5sHF7XGRi/rcbjuIr8fQhyHQUmxEAgMphN6mtzxhCdfj8N6ze95Gn8qxEuYZ7bcglOrOQOpWxP4f97vr8PWYkORMP2gyAdaW2mdf6BBmf0iSZZkqzjN4nT1S1JUWV4lq98h7OOrWWVV5FWRhixL2CxTfnWBhDDnn/p9tDlGosU+ke/QR/IFj8DAefSjC858xP3/dtGSM6N9fQtPi2UnKei9EGbU/pqIcY8at4udbzxzb0B/MLqV2x9xuAVVtuT8F8PaGgXH0XfGyn8W5/HdnQuL8qyh49XdGEU3E/Hfob6VupF6ezvixydLU8yfXRS+pY7sRM13pMkeF6xKGLix79HuoTrM8/xvAAAA//9JcufCjQcAAA
      objectset.rio.cattle.io/id: helm-app
      objectset.rio.cattle.io/owner-gvk: /v1, Kind=Secret
      objectset.rio.cattle.io/owner-name: sh.helm.release.v1.rancher-monitoring-crd.v4
      objectset.rio.cattle.io/owner-namespace: cattle-monitoring-system
    creationTimestamp: "2025-12-15T04:23:06Z"
    generation: 8
    labels:
      catalog.cattle.io/cluster-repo-name: rancher-charts
      objectset.rio.cattle.io/hash: 34b63f62fa3b930bbeb26c56d297a28c3f1b9ec0
    name: rancher-monitoring-crd
    namespace: cattle-monitoring-system
    ownerReferences:
    - apiVersion: v1
      blockOwnerDeletion: false
      controller: true
      kind: Secret
      name: sh.helm.release.v1.rancher-monitoring-crd.v4
      uid: dcbc0446-2f05-44a4-8ac1-7e54a36ff7e1
    resourceVersion: "65949776"
    uid: 7b33918d-d817-4df5-aa4e-e3aa7043566a
  spec:
    chart:
      metadata:
        annotations:
          catalog.cattle.io/certified: rancher
          catalog.cattle.io/hidden: "true"
          catalog.cattle.io/namespace: cattle-monitoring-system
          catalog.cattle.io/release-name: rancher-monitoring-crd
        apiVersion: v2
        description: Installs the CRDs for rancher-monitoring.
        name: rancher-monitoring-crd
        type: application
        version: 108.0.0+up77.9.1-rancher.6
    helmVersion: 3
    info:
      description: Upgrade complete
      firstDeployed: "2025-12-15T04:23:05Z"
      lastDeployed: "2025-12-15T06:43:41Z"
      readme: |-
        # rancher-monitoring-crd
        A Rancher chart that installs the CRDs used by rancher-monitoring.

        ## How does this chart work?

        This chart marshalls all of the CRD files placed in the `crd-manifest` directory into a ConfigMap that is installed onto a cluster alongside relevant RBAC (ServiceAccount, ClusterRoleBinding, ClusterRole, and PodSecurityPolicy).

        Once the relevant dependent resourcees are installed / upgraded / rolled back, this chart executes a post-install / post-upgrade / post-rollback Job that:
        - Patches any existing versions of the CRDs contained within the `crd-manifest` on the cluster to set `spec.preserveUnknownFields=false`; this step is required since, based on [Kubernetes docs](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#field-pruning) and a [known workaround](https://github.com/kubernetes-sigs/controller-tools/issues/476#issuecomment-691519936), such CRDs cannot be upgraded normally from `apiextensions.k8s.io/v1beta1` to `apiextensions.k8s.io/v1`.
        - Runs a `kubectl apply` on the CRDs that are contained within the crd-manifest ConfigMap to upgrade CRDs in the cluster

        On an uninstall, this chart executes a separate post-delete Job that:
        - Patches any existing versions of the CRDs contained within `crd-manifest` on the cluster to set `metadata.finalizers=[]`
        - Runs a `kubectl delete` on the CRDs that are contained within the crd-manifest ConfigMap to clean up the CRDs from the cluster

        Note: If the relevant CRDs already existed in the cluster at the time of install, this chart will absorb ownership of the lifecycle of those CRDs; therefore, on a `helm uninstall`, those CRDs will also be removed from the cluster alongside this chart.

        ## Why can't we just place the CRDs in the templates/ directory of the main chart?

        In Helm today, you cannot declare a CRD and declare a resource of that CRD's kind in templates/ without encountering a failure on render.

        ## [Helm 3] Why can't we just place the CRDs in the crds/ directory of the main chart?

        The Helm 3 `crds/` directory only supports the installation of CRDs, but does not support the upgrade and removal of CRDs, unlike what this chart facilitiates.
      status: deployed
    name: rancher-monitoring-crd
    namespace: cattle-monitoring-system
    resources:
    - apiVersion: v1
      kind: ServiceAccount
      name: rancher-monitoring-crd-manager
      namespace: cattle-monitoring-system
    - apiVersion: v1
      kind: ConfigMap
      name: rancher-monitoring-crd-manifest
      namespace: cattle-monitoring-system
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRole
      name: rancher-monitoring-crd-manager
    - apiVersion: rbac.authorization.k8s.io/v1
      kind: ClusterRoleBinding
      name: rancher-monitoring-crd-manager
    version: 4
  status:
    observedGeneration: 8
    summary:
      state: deployed
- apiVersion: management.cattle.io/v3
  kind: Cluster
  metadata:
    annotations:
      provisioner.cattle.io/encrypt-migrated: "true"
    creationTimestamp: "2025-09-10T18:18:00Z"
    generation: 3
    labels:
      provider.cattle.io: rke2
    name: local
    resourceVersion: "10981"
    uid: 6bac707e-b6c8-4389-80f2-4741311f516c
  spec:
    agentImageOverride: ""
    answers: {}
    clusterSecrets: {}
    description: ""
    desiredAgentImage: ""
    desiredAuthImage: ""
    displayName: local
    dockerRootDir: /var/lib/docker
    enableNetworkPolicy: null
    fleetWorkspaceName: fleet-local
    internal: true
    localClusterAuthEndpoint:
      enabled: false
    windowsPreferedCluster: false
  status:
    agentImage: ""
    aksStatus:
      privateRequiresTunnel: null
      rbacEnabled: null
      upstreamSpec: null
    appliedEnableNetworkPolicy: false
    appliedSpec:
      agentImageOverride: ""
      answers: {}
      clusterSecrets: {}
      description: ""
      desiredAgentImage: ""
      desiredAuthImage: ""
      displayName: ""
      enableNetworkPolicy: null
      internal: false
      localClusterAuthEndpoint:
        enabled: false
      windowsPreferedCluster: false
    authImage: ""
    capabilities:
      loadBalancerCapabilities: {}
    conditions:
    - status: "True"
      type: Ready
    - lastUpdateTime: "2025-09-10T18:18:15Z"
      status: "True"
      type: Connected
    driver: imported
    eksStatus:
      generatedNodeRole: ""
      managedLaunchTemplateID: ""
      managedLaunchTemplateVersions: null
      privateRequiresTunnel: null
      securityGroups: null
      subnets: null
      upstreamSpec: null
      virtualNetwork: ""
    gkeStatus:
      privateRequiresTunnel: null
      upstreamSpec: null
    provider: rke2
kind: List
metadata:
  resourceVersion: ""
